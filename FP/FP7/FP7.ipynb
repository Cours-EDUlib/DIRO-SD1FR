{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "FP7.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbM0CEI41HOl"
      },
      "source": [
        "\n",
        "# Formation Pratique 7 : Régression, Biais et Variance\n",
        "\n",
        "Au cours de cette démonstration nous allons nous intéresser aux problèmes de régressions et nous en servir pour illustrer des notions vues en cours : capacité des modèles, sur-apprentissage, sous-apprentissage, biais et variance. Cette première cellule de code importe des fonctions de visualisations dont nous nous servirons, vous pouvez les consulter sur https://github.com/charlesGE/MOOC_DataSciences/FP7/utilities. \n",
        "Certaines parties de cette démonstration ont été adaptées des notebooks de Laurent Charlin, avec son autorisation.\n",
        "\n",
        "# <font color=”blue”>  Pratical Formation 7 : Regression, Bias and Variance \n",
        "\n",
        "During this demonstration we will focus on regression problems, and use them to illustrate some notions seen in class : model capacity, overfitting, underfitting, bias and variance. This first cell of code will import some visualization functions that we will use, you can consult them at https://github.com/charlesGE/MOOC_DataSciences/FP7/utilities. \n",
        "Some of the materials have been adapted from the notebooks of Laurent Charlin, with his permission. </font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah_TemlA1HOn"
      },
      "source": [
        "#@title\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import cm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "!rm -rf DIRO-SD1FR\n",
        "!git clone https://github.com/Cours-EDUlib/DIRO-SD1FR/\n",
        "import sys\n",
        "sys.path += [‘DIRO-SD1FR/FP/FP7/']\n",
        "\n",
        "# We import several home functions in order to create graphics\n",
        "from utilities import scatter_plot, plot_polynomial_curves, \\\n",
        "                      plot_optimal_curve, train_poly_and_see, MSE\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA83UVus1HOn"
      },
      "source": [
        "## 7.1 Capacité d'un modèle\n",
        "\n",
        "Informellement, la capacité d'un modèle peut être definie comme le nombre de functions qu'un modèle peut apprendre. Un modèle de faible capacité pourrait parfaitement apprendre (c'est à dire obtenir une erreur d'entrainement de 0) un plus petit nombre de functions qu'un modèle de haute capacité.\n",
        "\n",
        "Les modèles de haute capacité sont plus propices au **sur-apprentissage**. Le sur-apprentissage arrive quand l'écart entre les erreurs d'entrainement et de test est grand, ou en d'autres termes quand les modèles mémorisent les proprietés des données d'entrainement qui ne sont pas utiles (i.e. ne se généralisent pas) pour faire des prédictions sur des données de test.\n",
        "\n",
        "Intuitivement, quand deux modèles apprennent les données d'entrainement aussi bien, c'est générelament celui de plus faible capacité qui géneralisera le mieux, c'est à dire qui obtiendra le moins d'erreur de test. C'est un bon exemple du https://en.wikipedia.org/wiki/Occam%27s_razor\n",
        "\n",
        "## <font color=”blue”> 7.1 Model capacity \n",
        "<font color=”blue”>\n",
        "Informally, the capacity of a model can be defined as the number of functions a model can fit. Lower-capacity models would be able to perfectly fit (i.e. obtain 0 train error) fewer functions than higher-capacity models. \n",
        "\n",
        "Higher-capacity models are generally more prone to **overfitting**. Overfitting occurs when the gap between the test and training error is large, or in other words, when models memorize properties of the training set that are not useful for (i.e. do not generalize to) performing predictions on a test set. </font>\n",
        "\n",
        "Intuitively, when two models fit the training data equally well, usually the model with less capacity will generalize better, i.e. have lower test error. This is a good illustration of https://en.wikipedia.org/wiki/Occam%27s_razor) </font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5eQxMqJ1HOn"
      },
      "source": [
        "### 7.1.1 Génération de données\n",
        "\n",
        "Commençons par simuler quelques données ! Dans cette section, chaque observation $y$ est générée suivant ce modèle:\n",
        "\n",
        "$$ y = x \\cos(x / \\gamma) + \\epsilon$$\n",
        "\n",
        "Où $y \\in \\mathbb{R}$ est la sortie, $x \\in \\mathbb{R}$ sont les caractéristiques, $\\gamma$ est la periode de la fonction cosine et $\\epsilon$ le bruit aléatoire tel que $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ où VOUS définissez $\\sigma$.\n",
        "\n",
        "### <font color=”blue”> 7.1.1 Data generation\n",
        "\n",
        "<font color=”blue”> Let's first simulate some data! In this section, every observation $y$ is generated according to the following model:\n",
        "\n",
        "<font color=”blue”> $$ y = x \\cos(x / \\gamma) + \\epsilon$$\n",
        "\n",
        "<font color=”blue”> where $y \\in \\mathbb{R}$ is the output, $x \\in \\mathbb{R}$ are the features, $\\gamma$ is the period of the cosine function and $\\epsilon$ is the random noise such as $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ where $\\sigma$ is defined by YOU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Loi6I2jk1HOo"
      },
      "source": [
        "def data_simulation(sample_size, scale, period, variance):\n",
        "    \n",
        "    x = np.random.uniform(-scale, scale, sample_size)\n",
        "    x.sort()\n",
        "    noise = np.random.normal(0, variance, sample_size)\n",
        "    y = x * np.cos(x / period) + noise\n",
        "    \n",
        "    return x, y"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYnEVxac1HOo"
      },
      "source": [
        "Quand c'est possible, il est toujours bon de visualiser les données (afin d'obtenir une certaine intuition à leur propos). Utiliser la cellule suivante et jouez avec les valeurs de **sample_size**, **variance**, **scale** et **period** pour comprendre comment elles affectent nos données. Les données d'entrainement sont indiquées en noir, celles de test en rouge.\n",
        "\n",
        "<font color=”blue”> Whenever it is possible, it is always a good idea to visualize the data (in order to get some intuition about them). Use the following cell and play with the values of **sample_size**, **variance**, **scale** and **period** to understand how they affect our data. Training data are indicated in black, while test data are red."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3mCibfrV1HOo",
        "outputId": "332263f7-1b05-4e0f-92c3-34112d4bce6e"
      },
      "source": [
        "sample_size = 150\n",
        "variance = 10   # Variance of the Gaussian noise\n",
        "scale = 100   # Range\n",
        "period = 6   # Simulation are based on cosine function (see data_simulation function)\n",
        "\n",
        "x_train, y_train = data_simulation(int(.7*sample_size), scale, period, variance)\n",
        "x_test, y_test = data_simulation(int(.3*sample_size), scale, period, variance)\n",
        "\n",
        "plt = scatter_plot(x_train, x_test, y_train, y_test)   # The scatter_plot function is in the utilities script"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4MAAAE9CAYAAACxyLfWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dfXRc9X3n8c9XErgGO3UABxwwyOJQGau7JeBDk1GbbRkID1kV05YcclKHNm0oZ0m1bU83hSRnlqNmu6wJe3a9S0JoeUjIE0l3eZiWhOLpQk80oYnp5sFPWsyAweCCQwK1wUAl/faPe8ceyTOjmdF9vu/XOTqjuRpJP925uvd+f7/v7/sz55wAAAAAAPnSF3cDAAAAAADRIxgEAAAAgBwiGAQAAACAHCIYBAAAAIAcIhgEAAAAgBwiGAQAAACAHBqIuwFhOumkk9zg4GDczQAAAACAWDzxxBM/ds6tbPa1TAeDg4OD2rp1a9zNAAAAAIBYmNmeVl8jTRQAAAAAcohgEAAAAAByiGAQAAAAAHKIYBAAAAAAcohgEAAAAAByiGAQAAAAAHKIYBAAAAAAcohgEAAAAAByiGAQAAAAAHKIYDBDKpWKRkdHValU4m4KAAAAgIQjGMyQUqmkarWqUqkUd1MAAAAAJBzBYIZMTEyoUChoYmIi7qYAAAAASLiBuBuA4BSLRRWLxbibAQAAACAFGBkEAAAAgBwiGAQAAACAHCIYBAAAAIAcijUYNLM7zewlM9vWsO0EM3vEzJ70H9/ubzcz22xmu83sh2Z2bnwtBwAAAIB0i3tk8G5Jl8zbdr2kinPuLEkV/7kkXSrpLP/jGkmfi6iNAAAAAJA5sQaDzrm/l/STeZsvl/QF//MvSNrQsP2LzvO4pBVmtiqaliYLi8sDAAAAWKy4RwabOdk5t0+S/Md3+NtPlfRcw+v2+ttyZ3x8XNVqVePj43E3BQAAAEBKJTEYbMWabHNHvcjsGjPbamZb9+/fH0GzAAAAACB9khgMvlhP//QfX/K375W0uuF1p0l6Yf43O+dud86td86tX7lyZeiNjcPmzZtVKBS0efPmBV9LSikAAACAZpIYDD4o6Wr/86slPdCw/cN+VdF3S3q1nk6aN8ViUZOTkyoWiwu+tlQqqVqtqlQqRdAyAAAAAGkR99ISX5X0HUnDZrbXzH5X0k2SLjKzJyVd5D+XpIck1STtlvQXkv5dDE1OnYmJCRUKBU1MTMTdFAAAAAAJYs4dNe0uM9avX++2bt0adzMAAAAAIBZm9oRzbn2zryUxTRQAAAAAEDKCQQAAAADIIYJBAAAAAMghgkEAAAAAyCGCwaSr1aSREWlgwHus1Rb9I1l7EAAAAADBYALNCdbGxqRdu6SZGe9xbKyz72uDtQcBAAAAEAwm0JxgbWpKmp31vjA76z3v5PvaYO1BAAAAAASDCTQnWBselvr8t6mvz3veyfe1USwWNTk5qeKaNYGnoAIAAABIBxadT7pazUsNnZryAsFyWRoa6v11jUZGvNTT2Vkv0Fy7Vtq+PZy/AwAAAEDk2i06TzCYFb0EdgMD3lzEuv5+aXo63HYCAAAAiEy7YJA00azoYm7hYV2koAIAACC7qDafTwSDWdFLYFcueyOI/f3eY7kcbhsBAACQSFSbzyeCwQRZVI9ML4Hd0JCXSjo97T0uNMcQAAAA6dVm/WqqzecTcwYTZHR0VNVqVYVCQZOTk3E3BwAAAFlC8cBcYs5gSjT2yJC3DQAAgED1UmMCmUYwmCCH1/8rFsnbBgAAQHNt0j3bongg5iEYTCjytgEAANDU2JiX7jkz4z2OjXX2fRQPxDzMGQQAAADShLWi0QXmDAIAAABZQbonAkIwmCAUjQEAAMCCSPdEQAgGIzY/4Gt8HlfRGIJQAACAFGGtaASEOYMRm7+WYOPziYkJ3fbxj+uul1/Wsr17vSH/cjn0f3DWNwQAAACyiTmDCTK/Smjj82KxqG+88YaWPfdc99WhAmwTAAAA8o3MsXxgZDBBKpWKfuXCC9XfuJHqUAAAAIgYmWPZwchgSpRKJe2SdLhQcETVoej5AQAASLao79fIHMsHgsEEmZiY0I3nnqtDZ5wRSnWoVieRuArXAAAAoDOh3K/VatLIiLdu4ciI99xXLBY1OTmpYrEY3O9D4pAmmiOthvvrlUzr8xYBAACQLKHcr42MeDUqZme9jLS1a73qpMiUdmmiBIM5QtAHAACAwwYGvKKFddSqyKR2weBA1I1BfIrFIkEgAAAAPMPDc0cGI6hVgWRhziAAAACQR+WylxoaQq0KpAPBIAAAAJAQkVYNHRry5ghOT3uPQ0Ph/04kCsEgAAAAkBBUeUeUCAYBAACAhGB9P0SJYBBAa23WHwIAAMFru75fh9flqBeoR3oRDOYNN/foxtiYV2VsZsZ7HBuLu0UAAORXh9dlUk3RKYLBvOnkJELAiLqpKa/ctOQ9Tk3F2x4AAPKsw+syqaboFIvO500ni4uOjMxdc2btWq/CFPKHYwEAgOTguowetFt0npHBvBke9k4eUuvFRRkNQh3rDwEAkBxclxGwgbgbgIiVy15q6NSUFwg2O4kMD8/tdWoWMCIf6usPAQCA+HFdRsAYGcybThYXpdcpNxaqNkY1MgAAcoS6EblDMIijdRIwIhMWqjZGNTIASBY66VAXyrFAFfHcIRgEcmyhamNUIwOAZKGTDnWhHAvUjcgdgkEgx45a2HZeekhxzZrWC98CACLROAJEJx3qQjkWOik0iExhaQm0VKlUVCqVNDExQTCQF5SsBoDEGR0dVbVaVaFQ0OTkZNzNQZbVakcXGmS6UOqxtAR6QipKDvWQHsL8FQAIF6OBiAx1I3KHYBAtcfHJoR7SQ+g0AIBwHZXSX0flR8SIzuBsIBhESy0vPki8nk/QPSwrQqcBAMSEyo8I0UL3EnQGZwNzBoEMmjO/5J57yP8HgCwaGPACwbr+fi+9D+mXgLl7C81VpbZEejBnEMiZOaN19BwDQDZR+TG7EnDtXijzhwyybGBkEMg6eo4BIJsSMHqEkHDtRoBSOTJoZs+Y2Y/M7PtmttXfdoKZPWJmT/qPb4+7nVnCROCM6rDnuKv3n6IFABA/Kj9m1sHTTtPhUJBRX4QoscGg71edc+c0RLLXS6o4586SVPGfIyBMBM6I+YHarbd2VBSmq/c/AekrAABk1e+ceKJ2SV5A2GFBN6AXSQ8G57tc0hf8z78gaUOMbckcqkJmxPxA7brrOuo57ur9n78e4a5djBQCABCQazdt0jWFgh7dsoVRX4QqsXMGzexpST+V5CR93jl3u5m94pxb0fCanzrnWqaKMmcQuRTFPIORES8AnJ310lcGBrzfUX++dq138QIAAJ1LyTxQKommSyrnDEoadc6dK+lSSdeZ2Xs7+SYzu8bMtprZ1v3794fbQiCJoqguN389wpmZuSOFU1PB/04kDvOMASBgKZmGwdSi7EhsMOice8F/fEnSfZLOl/Sima2SJP/xpSbfd7tzbr1zbv3KlSujbDKQDD0sHN+1+UULKG+eS9wMAEDvmnaozZ+GkdDOVaYWZUcig0EzO97Mltc/l/Q+SdskPSjpav9lV0t6IJ4WAgkWR3W5KAJQJA43AwDQu6YdainpXGWNwexIZDAo6WRJ3zazH0j6rqS/cc59S9JNki4ysyclXeQ/B/ItCcs8UN48l7gZAOYidRrdaNqhRucqIpbYAjJBoIAMcmF+MReKtwBALEZHR1WtVlUoFDQ5ORl3cwBAUnoLyADoRErmFwBA1pE6DSBtBuJuAIBFGh6eOzKY0PkFAJB1xWKRtGkAqcLIIJB2zC8AAABADwgG0TUmyCcMxVsAIF8WUzgsCUXHACQGwSC6xtpiAADEqM3C5At22KZkUXMA0SAYRNeYII/D6GEGgOi1KRy2YIctRccANCAYRFcqlYpKpZImJiaYJA96mAEgDm0WJl+wwzYli5oDiAbBILpCiijmmNfD7Hbtirc9AJBG3WZZtCkcViwWNTk52brDlqJjABoQDKIrpIhijoYe5hlJzyxZcvhLFBoCgA51m2WxmMJhFB1DQLjOZ4M55+JuQ2jWr1/vtm7dGnczgOyq1aSxMbldu/TMkiV64fOf1+jGjZKk0dFRVatVFQoFTU5OxtxQAEiwgQEvEKzr7/eCNSDBuM6nh5k94Zxb3+xrjAwC6J3fw2wzM1rz+uuHA0GJUWQA6Bjz+JBCXOezgZFBAACAOPlZFpqa8gLBcjmU9E2KwAH51G5kkGAQAAAgB0jrA/KJNFEAAICcI60PwHwDcTcAAAAA4SsWi6SHApiDkUEgpSjpDAAAgMUgGARSqlQqqVqtqlQqxd0UAAAApBDBIJAGtZo0MuKtRTUyItVqzP0AAADAolBNFEiDkRFp1y5pdtZbg2rtWmn79rhbBQAAgISjmiiQdlNTXiAoSbOzcrt2xdseYJGY8wog63J1nmuSwYR0IBgEEmDBC8bwsDciKGlG0jNLlkTXOCAEzHkFFiesQCNXAUzIcnWeGxvzMphmZrzHsbG4W4QOEQwCCbDgBaNcltaulevr07NLl+qFz38+2gZ2gRsJdII5r8DihBVo5CqACVmz81xmr5HzMpg0NRVve9A551xmP8477zwHpMGWLVtcoVBwW7Zsibspi1YoFJwkVygUmn49S39rmiR5vye5bUBShfV/w/9juBa6RqbWunXO9fU5J3mP69bF3SI0kLTVtYiXKCADIFCVSkWlUkkTExNNFzceHR1VtVpVoVDQ5ORkDC3MpyTv9yS3DQCC1HiNlNT2epkqtZqXGjo15U1tKZeloaG4WwUfBWSAFMtaSgnpgfFI8n5PctsAIEjFYlGTk5MqFovZSskdGvKqnE9Pe48EgqnByCCQcGkbNUlbexEAeoS7stDoOYB8yOq5IKt/V5q1GxkkGAQSLm0n1bS1FwFgHcyu0GECIMs4xyUPaaJAijWmlKRB0/ay/lBitEo7rm+/+eabu09LpopcV0iLBZBlnOPShZFBAOFj5CgxWvXYjoyMaMeOHVq6dKkOHTrUXY8u7y8AAInFyCCAeDFylBgL9diuXLmy+x5dfx1M9fd7j+Vyd41i5BhAxmWtGByyg5FBIAVSPw+PkaPEi/UY4/gAkHHMo0OcKCADpFilUtEVV1yhAwcOpPciQrVJtDMwIM3MHHne3++VJweAjEh9py5SjTRRIMVKpZIOHDig5cuXp3cyNusPoZ3hYW9EUPIeh4fjbQ/QSsZTmhtTGVt9jt6krRgc8oORQSDhstCbmIW/ASFi5BhpkfGU5sZURklNP09ldgqQc6SJAogVcyXCQZANRCzjKc2N5xRJTT/nXAOkD2miAGK1YcMGLV++XBs2bDiyMePpVlEolUqqVqsqlUpxNwXIh4ynNDemMrb6HFgs0o6ThWAQSJoMBkn333+/Dhw4oPvvv//IxrExL91qZsZ7HBuLr4EpxcK+QMQWu4xKhnBD3z32mYeOzGQhGEQgOMEF5+AFF2hmx45MBUlNgxbWHly0IHrrK5WKrjzvPB0cHMxUBwQQipwUw+rkms4NfffYZx46MhPGOZfZj/POO88hGoVCwUlyhUIh7qak3rTkXONHf3/cTQrHunXO9fV5f2Nfn/cckfvNc891hyQ3Wz/e0v5ePPWU1/7+fu/xqad6ew3QxpYtW1yhUHBbtmyJuymh6OSanvV9EAb2GeIiaatrES8xMohA0MsTnENnnKHD5QkyOCflsFtv9UaiJO/x1lvjbU9O3fXyy1oiyeob/FHa1I72d5J+nOMU5dS+rwmT9RGeTq7pzCPsHvsMSUQwiEBwggvOsr/7O/WvW5f9OSnXXXekCt/0tPcckVu2d++RQLBueDjym91ugpS2r+0k/bibFOWMzeHNehATlax3gHJNB/KDYBBImpzMSWHOYGdCH8kZHpasIRw89lipXA79Znf+39VNkNL2tZ1Ue+ymImQSRhEDDEizHsREpVWwxMgrgNRplT+ahQ/mDAIJxpzBjoQ+Hzfo+XMd/rz5f1c3c2navjboOYP9/fHP4Y3qf4W5lIvG/HkASaQ2cwZZdB5APGo1b5RlasobmSmXszsKugipW1h+ZMQbQZud9Ubd1q71RrjnSc3f1eHfE6qoFjpPwt+acqk5rgHkSrtF5wkGAQDBiSpwiUoSOi2iCtKy9t4BACS1DwaZMwgACZeqeUjdzMeLwmLn2yVhDm9UC50n7b1DJFJ1fgEQOIJBAEi4VFWAjCpw6VQSCsBIiwtKowpIk/beJUiWA6ZUnV8yKsvHF5KvZTBoZg+Z2WB0TQEANJOqCpBJGElr1EPV2lBuzMbGpJ07vaB0xw7p7LOTt0xF0t67CHT6Xmc5YErV+SWjsnx8IfnajQzeLelvzeyTZnZMRO0BkBcZW78tTKz5tQg9pD7Wb8zGx8eDCwqnprx6pHVvvZWrxe6TqtOb8CwHTCyTEb8sH19IvpbBoHPu65LeJeltkraa2Z+Y2R/XPyJrIZAxXGB9SUnfy4k8HHdN/8YeUh/rN2aSguutHx7WUeXaWFszdp3ehOexQ4bRqujk8fhCciw0Z/BfJL0maYmk5fM+YmFml5jZlJntNrPr42oH0CsusD4WnY9UmMddUgLNpn9js9THBUal6zdmmzdvDq63vlzWv5gdCQgp0JII3IS3xmgVkBOtFiCUdImkHZJuknRcq9dF+SGpX9JTkoYkHSvpB5LWtXo9i84jibpZXDvTWHQ+UmEed0lZaLvjvzGmY+/bX/yiqy1d6mbrvzOmRd05BwFAvqjNovPtRgY/KelK59z1zrnXwwtHu3K+pN3OuZpz7i1JX5N0ecxtArpCT7SvRfpeUkaZEqXX+ZUN31ccH9fkPfeEctxFPYLQ6hjp+H8rplHp0Y0bteb112UzM8EWaKnVdHBwUDNmOjg4uODxEWp2AnOBASBVUrXovJn9pqRLnHO/5z/fKOkXnXMfa/Z6Fp0H0md0dFTValWFQkGTk5NxNycZel10PKrFyiO26GMka/tlZEQzO3aoX9KMpP5169r+PZVKRaVSSRMTE8F3DmRt3wJABmRp0Xlrsm1ONGtm15jZVjPbun///oiahfkqlYpGRkY0MjLCCA+60u0oUy5GEnsdycrovMxFj0TOG5WevP76I8dQGke2pqbU73/a7z9vpz6CKin4/52MHnMAkFmt8keT+CHpPZIebnh+g6QbWr2eOYPxqc8hUgLmESG9OpnblJT5aqHqdY4b8zI7MucYSuM+67HNofzvpHH/4SjMKwWyRT3OGUyi70k6y8zWmNmxkq6S9GDMbUITExMTWrdundatW0clsgXkYmSrR53MbcpFxbselkdY1PflzJxjKI0jWz2+z6H873DMZQJVr4H8SNWcQUkys8sk/Td52TB3Ouf+U6vXMmcQacAcudZCndsENMOct3yp1bw1TqemvKU+yuXgCvukGOdeIFuyNGdQzrmHnHM/55w7s10gCKRFLka2ekTlVUQu7SNbaZzz2IHQMijGxrzgf2bGexwbC/bnpxTnXgQmo+ekLEndyGA3GBkE0o3eaaBzlUpFQ2NjOuONN9TnXKZGNkPLoBgY8ALBuv5+aXo6uJ8P5B3ZFomQqZFBAPnBvBXkUa+jYKVSSasPHfICQSk9cx5baNwPoWVQDA97N6iS9zg8zDxuIEhpnIedMwSDABJrw4YNWr58uTZs2BB3U4DI9NoJMjExoeeWLtVs48b+/tSmZTXuh8DTFuupa7t2eaODDWnBdEKhHToLutSkwwXJQjAIILHuv/9+HThwQPfff3/cTQEi0+soWLFY1Jpt29R37LFHNk5Pp3YeXNP9ENT8o/pcwdlZbx8ND3upa0NDzOPuUt6CIzoLupT2edg5wJxBAInFnEGkRaKO1QzPgzs4OKile/aoX1rc/KMM76MoNB7v9eAoLxWxK5WKxsfHJUmbN2+O//8d6ABzBgGkEhXtEKXFjHAkarQgw2lZhwNBaXHzjzK8j6LQeLznbSS1WCxqxYoV2rFjRzL+34FFIhgEApa3lBl0j2MkmRYT0CXqhjjDaVmHzjhDh8fzFhPEZXgfRaHxeM9jp12i/t+BRSJNFAgYi8hjIfOPkUSlGOYY70MKsEg8AHSNNFEgQvQYYiHzj5FOR6QYUQxXLkY4QloAOpJjk0AQAALHyCAAxKzTESlGnaOVyZHCkBaAjuTYZPFqIJMyea5NGEYGASDBOh2RYtQ5WokqChOUHheAXmjkL5Jjk8WrE61+jEzec08oo8/Irkyea1OEYBAAUiIXaYwh6jaVMZPBd49VNBe6Wevq2Ow1VZUKoIlU/78aHx9XtVrVO3//970R3JkZ7zGl61wiOpk816YIwSAAIPMqlYquuOKKrnqfMxl891hFs9XNWk9zBesLvncbLFABNJHqHQWSVCgUNPjmm4zgonO1morj45r8h39QcXyckeQYMGcQAJB59Tlty5cv13333ZetAC9GPc0VZMH3TDlqvhdzO9ENjpdIMGcQsWrVc5z1yohZ//sQgpAqPeLIyBaBYLB6Su8i3TNTjhpBZwQX3WAucOwYGUToWvUcZ70yYtb/PiwePerIJZaIAFDHdS8SjAwiVq16jrM+YXjTtdeqtnSpvv3444zyoKmjinLQQ4o8GBrybvamp71HAkEgvxhJjh3BIELXqghDJoszNBi96SatefNN2ewsFdXQ1FEdIqTPIYHCTnknpR7IMTqHYkeaKBAWiiSgW6TPIYHCTnknpR4AwkWaKBAHRnnQocMjI08/TQ8pEifslP6sTxlAujFyjaxjZBAIC6M86BAjIwCQTJyfkQWMDAJxIA8eHWJkBAgYy7QgIJyfkXWMDAIAgGyhXD2QeEctr4TQMDIIAAAiE/s8K5ZpARLvqOWVEAuCQQAAEKjYb/Io4JVcpPDCRwpuMhAMAki92EchAMwR+00eC1nHyw/4XH+/nj7uOE3ec8+Rr42NeSm8MzOswZtzWV9vOi2YMwgg9aj2BgAJ0jBnc0bSs0uXas3rr3tfYw1eIHLMGQSQaXGNQjAimU28r8AiNczZ7Jc0+OabR75GCi+QKASDAFKv21SToG72Y58XhVDwvgKLNC/gs7Vrj3yNFF4gUQgGAeROUDf7sc+LQih4X4FFahfwsQYvkCjMGQSQO5VKRbd9/OO66+WXtWzvXq8Xu1zmpgQAAGQOcwYBwFdf5Paul1/WsueeO6qiHfPFACAanG+B+BEMAsiGDteuqqeILt2zp+mi1MwXA4BocL5FIzoH4kEwiNjwT49Adbh2VX0+2KEzzmha0Y75YgAQjVSebzvseET36ByIB3MGERvWhkOgul27qlbzAsapqdDnDNZTUycmJlhcFwDSrGENRfX1eQVytm+Pu1WZwLUyPO3mDBIMIjb80yNQCb5A0/GB3Iqw0wWIRLcdj0ACUEAGidTt2nBAWwleuyqVqVBAEDpM3waSoKPpK/PWUKxPMQDSimAQQCodddEOcO2qoOez0vGB3JqaalqoCUiijuasJbjjEegFwSCAVApzojmT2IGAMIqCFOkoiyPAjkcgCQgGgYBRJTUaQadeNr5vpHUCAWEUBSlCFkdycC8VHQrIAAGjWEg68b4BAJAMXJODRQEZIEKMKqUT7xsAAMnANTk6BINIvLSlCpBmkk7N3re0HXsAAA/n73TjXio6BINIPIp5IEztbhg49gAgArWat1bswID3WKst+kfe9vGP6/ZqVb9y4YWB/UwgiwgGkXikCiBM7QI+jj0AiEAI61He9fLLWiupX2KNS6ANgkEkXpJTBUhDSb92AV+Sjz0AyIwQ1qNctnevFwh2+TO5riNvqCYKLALVrgAAWKSREW/0bnbWW49y7VpvDb8YfibXdWQR1USBkJBGiE7R2wwALYSxHmWPP5PrOvKGkUEAuVKpVFQqlTQxMRFp+ie9zQAQkVrNmyM4NSUND3uB4NBQ3K1Ch+K6TmcZI4MA4IurQii9zQAQkRAK0iA6VPKOVuKCQTO70cyeN7Pv+x+XNXztBjPbbWZTZnZxnO0EkE5xBWUUowGAiHRakKZW08HBQc2Y6eDgIMtPJASdp9FKXJqomd0o6aBz7jPztq+T9FVJ50t6p6Qtkn7OOTfT6meRJgpgMUhVAYAU6rR4zMiIZnbsUL+kGUn969YtvnANkEBZSRO9XNLXnHNvOueelrRbXmAIAKEgVQUAUqjT4jFTU4eXn+j3nwN5k9Rg8GNm9kMzu9PM3u5vO1XScw2v2etvA4BQkKoCACk0NOSN8E1Pe4+tiscMD3sjh5L3ODwcXRuBhIglGDSzLWa2rcnH5ZI+J+lMSedI2ifplvq3NflRR+W4mtk1ZrbVzLbu378/tL8BOVWreeknAwPeI/MLMo15fgCQYWEsaQGkTOLmDDYys0FJf+2c+3kzu0GSnHP/2f/aw5JudM59p9X3M2cQgQtjYVwAAAAgJKmaM2hmqxqeXiFpm//5g5KuMrMlZrZG0lmSvht1+5ANPS8A3mmFMgAAACDhEhcMStpkZj8ysx9K+lVJfyRJzrntkr4uaYekb0m6rl0lUaRPqwCt58CtjZ4LgzC/AAAAABmRuGDQObfROfevnHP/2jn3a865fQ1f+0/OuTOdc8POuW/G2U4Er1WANj4+rmq1qvHx8cB+V8+FQZhfAABAz8Lo4AXQu8QFg8ivKCs39lwYpNMKZQAA4Cg9ZeZ0WbyNgDO7eG+DRzCIxGgVoG3evFmFQkGbN2+OqWUAACAIPXX8jo15xdtmZrzHsbG2ASJrxGYX723wEl1NdLGoJppitZp3sp+a8ubllcuMwgEAkEcDA14gWNff790btKjuXalUVCqVNDExwdJAGcN725t21UQJBpFMDUs4zJppz8/8jGrlMv/4AADkTbNlnaamjg4Qp6fjayOQYKlaWgKQNGcJhz7ntPrQIVICAADIqnbzAstlHVy9WjOSDq5e7WULUd0bCATBIJKp4SQ/a6bnli6NpLAMAACIQbN5gXVDQ7r41FM1IOniU0/1po0sprp3lwVpgCwjGEQyNZzk+84+W2u2bYs8RZSKVQAARKQhI0izs97zBkcVnllMde92gSeQM8wZBFoYHR1VtVpVoWx+XOMAABE2SURBVFDQ5ORk3M0BACC7ms0L9AvCBK5ZQRrmGyLDmDOI3AhyNC/KdQ8BAMi1xaR9dqh+j3DwtNOOmm/Y7P6BDCHkASODyBRG8wAAQDP1e4TfPPdcfeONN+YsXzW6ceNR9w/cUyArGBlEbjCaBwAAmqnfI1y7adNR8w2b3T9wT4FupHUkmZFBZAYLkQIAAO4HEIckjyQzMohcKJVKqlarrEcIAECOcT+AOKR1JJlgEJmR1n9CAAAQHO4HEIdisajJycnUjUaTJgoAAAAg8UgB7g1pogAAAABSLZEpwLWat07mwID3WKvF3aKuEAwCAAAASLxEpgCPjUm7dkkzM97j2FjcLeoKwSAAAAAyKa3l/tFcIuflTU1Js7Pe57Oz3vMUIRhE7nGhAAAgPbq5bicyrRDZMjws9fkhVV+f9zxFCAaRe1woAABIj26u24lMK0S2lMvS2rVSf7/3WC7H3aKuEAwi97hQAACQHt1ctxOZVohIRJb5NTQkbd8uTU97j0ND4f6+gLG0BAAAAIBMGR0dVbVaVaFQ0OTkZNzNiRVLSwAAAADIDTK/OjMQdwMAAAAAIEjFYpH04A4wMggAAAAAOUQwCABAilx77bX6sz/7s7ibAQDJUatJIyPSwID3WKvl43cHgAIyAABEaHBwUH/5l3+pCy+8MO6mAEA2jIxIu3Z5i7739XlLPGzfnv3f3SEKyAAAkALT09NxNwEA0mdqygvGJO9xair439FqBDCK3x0igkEAACKyceNGPfvssxobG9OyZcu0adMmmZnuuOMOnX766brgggskSVdeeaVOOeUU/ezP/qze+973antDL/Nv//Zv61Of+pQk6dFHH9Vpp52mW265Re94xzu0atUq3XXXXbH8bQAQm+Fhb1RO8h6Hh4P/HWNj3gjgzIz3ODYW3e8OEcEgAAARueeee3T66aerXC7r4MGD+sAHPiBJeuyxx7Rz5049/PDDkqRLL71UTz75pF566SWde+65+tCHPtTyZ/7TP/2TXn31VT3//PO64447dN111+mnP/1pJH8PACRCueylZ/b3e4/lcvC/o9UIYBS/O0QEgwAANKhUKhodHVWlUonsd9544406/vjjtXTpUknSRz7yES1fvlxLlizRjTfeqB/84Ad69dVXm37vMccco1KppGOOOUaXXXaZli1bpqmUpSkBwKIMDXnz9KanvcehoeB/R6sRwCh+d4gIBgEAaFAqlVStVlUqlSL7natXrz78+czMjK6//nqdeeaZetvb3qbBwUFJ0o9//OOm33viiSdqYODIssHHHXecDh48GGp7ASB3Uj4C2ArBIAAADSYmJlQoFDQxMRHKzzezttu+8pWv6IEHHtCWLVv06quv6plnnpEkZbn6NwAkXspHAFsZWPglAADkR7FYVLFYDO3nn3zyyaq1WYfqwIEDWrJkiU488US9/vrr+sQnPhFaWwAA+cbIIAAAEbrhhhv06U9/WitWrNBf/dVfHfX1D3/4wzrjjDN06qmnat26dXr3u98dQysBIH0WNec75YvH94pF5wEAAACk3ujoqKrVqgqFgiYnJ7v75m4Wj6/VvKUlpqa8QjLlcqLTRll0HgAAAECmLWrOdzeLx7daczCFCAYBAAAApF6xWNTk5OThed9dpY12s3h8N4FjwhEMAgAAAMicrpYKarJ0RMtgspvAMeEIBgEAAABkTldpo02WjmgZTGZozUGCQQAAAACZMydttIdqoS2DyQytOcg6gwAAAACyrV70ZXb2SNGXVtVCffW5h/WRwTDXoI0LI4MAAAAAsq3Hoi9dzTtMIYJBAAAAANnWUPRlRtLB007r6NsWtVxFChAMAgAAAMg2v+jLjKRdkn7nxBM7+rb5y1VkDcEgAAARGhwc1JYtWxb1M+6++2790i/9UkAtAoAc8Iu+PLpli64pFHTtpk1xtygRKCADAAAAIBeKxWJmR/l6wcggAAAR2bhxo5599lmNjY1p2bJl2rRpkx5//HEVCgWtWLFCv/ALv6BHH3308OvvvvtuDQ0Nafny5VqzZo2+/OUva+fOnbr22mv1ne98R8uWLdOKFSvi+4MAAKlGMAgAQF0P61B145577tHpp5+ucrmsgwcP6kMf+pDe//7361Of+pR+8pOf6DOf+Yx+4zd+Q/v379drr72m8fFxffOb39SBAwdUrVZ1zjnn6Oyzz9Ztt92m97znPTp48KBeeeWVQNsIAJAqlYpGR0dVqVTibkqoCAYBAKirr0M1M3NkHaoQfelLX9Jll12myy67TH19fbrooou0fv16PfTQQ5Kkvr4+bdu2TYcOHdKqVas0MjISansAAJ6sLylRF0swaGZXmtl2M5s1s/XzvnaDme02sykzu7hh+yX+tt1mdn30rQYAZF6P61D1as+ePfrGN76hFStWHP749re/rX379un444/Xvffeq9tuu02rVq3S+9//fu3atSvU9gAAPFlfUqIurpHBbZJ+XdLfN240s3WSrpI0IukSSZ81s34z65d0q6RLJa2T9EH/tQAABKdhHSr19XnPA2Zmhz9fvXq1Nm7cqFdeeeXwx2uvvabrr/f6PC+++GI98sgj2rdvn9auXauPfvSjR/0MAMDizU8LbbekRJZSSGMJBp1zO51zzbpbL5f0Nefcm865pyXtlnS+/7HbOVdzzr0l6Wv+awEACI6/DpX6+73HcjnwX3HyySer5s9F/K3f+i2Vy2U9/PDDmpmZ0RtvvKFHH31Ue/fu1YsvvqgHH3xQr732mpYsWaJly5apv7//8M/Yu3ev3nrrrcDbBwB51E1aaJZSSJM2Z/BUSc81PN/rb2u1HQCA4PjrUGl62nscGgr8V9xwww369Kc/rRUrVujee+/VAw88oD//8z/XypUrtXr1at18882anZ3V7OysbrnlFr3zne/UCSecoMcee0yf/exnJUkXXHCBRkZGdMopp+ikk04KvI0AkDcdp4XWanr4+ec1Lenh558PvNBY1Mw5F84PNtsi6ZQmX/qkc+4B/zWPSvoT59xW//mtkr7jnPuS//wOSQ/JC1ovds79nr99o6TznXN/0OT3XiPpGkk6/fTTz9uzZ0/QfxoAAACAPBoZ8QqMzc560wnWrvU6DxPMzJ5wzq1v9rXQFp13zl3Yw7ftlbS64flpkl7wP2+1ff7vvV3S7ZK0fv36cCJdAAAAAPkTcaGxsCUtTfRBSVeZ2RIzWyPpLEnflfQ9SWeZ2RozO1ZekZkHY2wnAAAAgLyJoNBYlOJaWuIKM9sr6T2S/sbMHpYk59x2SV+XtEPStyRd55ybcc5NS/qYpIcl7ZT0df+1AAAAABCNCAqNRSm0OYNJsH79erd169a4mwEAAAAAsWg3ZzBpaaIAAAAAgAgQDAIAAABADhEMAgAAAEAOEQwCAAAAQA4RDAIAAABADhEMAgAAAEAOEQwCAAAAQA4RDAIAAABADhEMAgAAAEAOmXMu7jaExsz2S9oTdzuaOEnSj+NuRI6x/+PDvo8X+z8+7Pt4sf/jw76PD/s+Xkna/2c451Y2+0Kmg8GkMrOtzrn1cbcjr9j/8WHfx4v9Hx/2fbzY//Fh38eHfR+vtOx/0kQBAAAAIIcIBgEAAAAghwgG43F73A3IOfZ/fNj38WL/x4d9Hy/2f3zY9/Fh38crFfufOYMAAAAAkEOMDAIAAABADhEMhszMrjSz7WY2a2br533tBjPbbWZTZnZxw/ZL/G27zez66FudPWZ2r5l93/94xsy+728fNLNDDV+7Le62ZpGZ3Whmzzfs58savtb0/wDBMLObzWyXmf3QzO4zsxX+do79iHBOj46ZrTaz/2NmO/1r77/3t7c8ByE4/vX1R/4+3upvO8HMHjGzJ/3Ht8fdziwys+GG4/v7ZvbPZvaHHPvhMbM7zewlM9vWsK3p8W6ezf514Idmdm58LZ+LNNGQmdnZkmYlfV7Snzjn6ifHdZK+Kul8Se+UtEXSz/nf9v8kXSRpr6TvSfqgc25HxE3PLDO7RdKrzrkJMxuU9NfOuZ+Pt1XZZmY3SjronPvMvO1N/w+cczORNzKjzOx9kv7OOTdtZv9Fkpxzf8qxHw0z6xfn9MiY2SpJq5xz/2hmyyU9IWmDpA+oyTkIwTKzZyStd879uGHbJkk/cc7d5HeGvN0596dxtTEP/PPO85J+UdLviGM/FGb2XkkHJX2xfi1tdbz7QfgfSLpM3vvy351zvxhX2xsxMhgy59xO59xUky9dLulrzrk3nXNPS9ot74b4fEm7nXM159xbkr7mvxYBMDOTd1Pw1bjbAkmt/w8QEOfc3zrnpv2nj0s6Lc725BDn9Ag55/Y55/7R//yApJ2STo23Vbl3uaQv+J9/QV5wjnAVJT3lnNsTd0OyzDn395J+Mm9zq+P9cnlBo3POPS5phd95FTuCwficKum5hud7/W2ttiMYvyzpRefckw3b1pjZ/zWzx8zsl+NqWA58zE+NuLMhTYjjPVofkfTNhucc++HjGI+JP/r9Lkn/4G9qdg5CsJykvzWzJ8zsGn/byc65fZIXrEt6R2yty4+rNLfTm2M/Oq2O98ReCwgGA2BmW8xsW5OPdr2/1mSba7MdC+jwffig5p4g90k63Tn3Lkl/LOkrZva2KNudFQvs/89JOlPSOfL2+S31b2vyozjeu9TJsW9mn5Q0LenL/iaO/WhwjMfAzJZJ+l+S/tA5989qfQ5CsEadc+dKulTSdX4aHSJkZsdK+jVJ3/A3cewnQ2KvBQNxNyALnHMX9vBteyWtbnh+mqQX/M9bbUcbC70PZjYg6dclndfwPW9KetP//Akze0re3M2tITY1kzr9PzCzv5D01/7Tdv8H6FAHx/7Vkv6tpKLzJ4pz7EeGYzxiZnaMvEDwy865/y1JzrkXG77eeA5CgJxzL/iPL5nZffLSpF80s1XOuX1+WtxLsTYy+y6V9I/1Y55jP3KtjvfEXgsYGYzPg5KuMrMlZrZG0lmSviuvuMBZZrbG7925yn8tFu9CSbucc3vrG8xspT/RWmY2JO99qMXUvsyalxd/haR65a1W/wcIiJldIulPJf2ac+71hu0c+9HgnB4hf174HZJ2Ouf+a8P2VucgBMTMjveL9sjMjpf0Pnn7+UFJV/svu1rSA/G0MDfmZEBx7Eeu1fH+oKQP+1VF3y2vkOG+OBo4HyODITOzKyT9D0krJf2NmX3fOXexc267mX1d0g55qVvX1SsomtnHJD0sqV/Snc657TE1P2vm59BL0nslTZjZtKQZSdc65+ZPBsbibTKzc+SlRDwj6fclqd3/AQLzPyUtkfSId5+sx51z14pjPxJ+FVfO6dEZlbRR0o/MX0JI0ickfbDZOQiBOlnSff55ZkDSV5xz3zKz70n6upn9rqRnJV0ZYxszzcyOk1e5uPH4bnr9xeKZ2Vcl/Yqkk8xsr6T/KOkmNT/eH5JXSXS3pNflVXlNBJaWAAAAAIAcIk0UAAAAAHKIYBAAAAAAcohgEAAAAAByiGAQAAAAAHKIYBAAAAAAcohgEACAgJnZajN72sxO8J+/3X9+RtxtAwCgjmAQAICAOeeek/Q5eWtOyX+83Tm3J75WAQAwF+sMAgAQAjM7RtITku6U9FFJ73LOvRVvqwAAOGIg7gYAAJBFzrl/MbP/IOlbkt5HIAgASBrSRAEACM+lkvZJ+vm4GwIAwHwEgwAAhMDMzpF0kaR3S/ojM1sVc5MAAJiDYBAAgICZmckrIPOHzrlnJd0s6TPxtgoAgLkIBgEACN5HJT3rnHvEf/5ZSWvN7N/E2CYAAOagmigAAAAA5BAjgwAAAACQQwSDAAAAAJBDBIMAAAAAkEMEgwAAAACQQwSDAAAAAJBDBIMAAAAAkEMEgwAAAACQQwSDAAAAAJBD/x+Dh6rUXGdRWwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_ciuvY71HOp"
      },
      "source": [
        "### 7.1.2 Obtenir une intuition visuelle de la capacité des modèles\n",
        "\n",
        "Plus un modèle a une grande capacité, mieux il apprendra les données d'entrainement (attention cependant, bien apprendre les données d'entrainement ne mène pas automatiquement à une bonne généralisation). Ici, nous utilisons la [régression polynomiale](https://en.wikipedia.org/wiki/Polynomial_regression) (introduite au cours du module 6) pour apprendre les données d'entrainement. Le degré du polynome est un hyperparamètre important du modèle et plus le degré est élevé, plus le modèle aura une grande capacité.\n",
        "\n",
        "**Questions (exécutez la cellule de code suivante)**:\n",
        "1. Observez comment la courbe apprise évolue en fonction du degré du polynome.\n",
        "2. Préféreriez-vous apprendre les données avec un polynome de degré 20 ou 50 ?\n",
        "3. Laquelle de ces courbes devrait avoir la meilleur erreur de généralisation ?\n",
        "4. Le code affiche également l'erreur MSE (moyenne de l'erreur quadratique) pour chaque degré envisagé. Si on sélectionne le degré avec la meilleure MSE sur les données de test, cette MSE sera-t-elle une bonne estimation des performances de notre modèle sur de nouvelles données ? Si non, que pourrions nous changer pour estimer correctement les performances du modèle selectionné sur de nouvelles données ?\n",
        "\n",
        "### <font color=”blue”> 7.1.2 Getting visual intuition about models' capacity\n",
        "\n",
        "<font color=”blue”> The higher the capacity of the model, the better it will fit the training data set (caution though, fitting the training data well does not necessarily lead to good generalization). Here, we use [polynomial regression](https://en.wikipedia.org/wiki/Polynomial_regression) (introduced in module 6) to fit the training set. The polynomial degree is an important hyperparameter of the model and the higher the degree is, the higher the model capacity will be.\n",
        "\n",
        "<font color=”blue”> **Questions (execute the code cell below)**: \n",
        "1. Observe how the fitted curve behave with respect to their polynomial degree. \n",
        "2. Would you prefer to fit the data points with polynomial regression of degree 20 or 50?\n",
        "3. Which of these curves should have the best generalization error?\n",
        "4. The code also prints the MSE error for each considered degree. If we select the degree with best test MSE, will this MSE be a be a good estimation of how our model will perform on new data points ? If not, what could we do to estimate the actual performance of our selected model on new data points ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "-Kue6BXz1HOp"
      },
      "source": [
        "degree = [0, 1, 3, 5, 10, 20, 50]   # Maximal polynomial degree of the fitted curve: higher degree == higher capacity\n",
        "\n",
        "plot_polynomial_curves(x_train, x_test, y_train, y_test, degree, scale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqLqXEPfPpvH"
      },
      "source": [
        "#### Réponses (essayez d'abord de répondre par vous-même avant de regarder la réponse)\n",
        "\n",
        "#### <font color=”blue”> Answers (try to answer by yourself before looking at the answers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWk6HE7vQPmN"
      },
      "source": [
        "Lorsque le degré augmente, la courbe polynomiale associée ressemble à une interpolation polynomiale. En regardant les données (première figure, Section 7.1.1), on peut voir un maximum de 15 minimums / maximums locaux. Par conséquent, la regression polynomiale de degré 50 semble excessive (trop grande capacité). En revanche, la regression polynomiale de degré 20 semble bien suivre la distribution des données. C'est confirmé par la MSE qui indique qu'un degré de 20 mène à la meilleure généralisation.\n",
        "\n",
        "Attention cependant, car en choisissant le modèle avec les meilleures performances sur les données de test, on risque le sur-apprentissage des hyperparamètres. Ainsi, nous pourrions observer des performances différentes sur de nouvelles données. Pour obtenir une estimation fiable, il faudrait séparer le jeu de données en données d'entrainement, de validation et de test, puis sélectionner le degré du polynome basé sur les performances sur les données de validation, et on pourrait alors correctement estimer les performances du modèle choisi en utilisant les données de test, comme vu dans le module 7.\n",
        "\n",
        "<font color=”blue”> As the degree increases, the associated polynomial curve looks like a polynomial interpolation. Looking at the dataset (first figure, Section 7.1.1), we can see at first sight a maximum of 15 local minima/maxima. Thus, polynomial regression with degree 50 seems overkill (too much capacity). On the other hand, polynomial regression with degree 20 seems to follow well the distribution of data. This is confirmed by the MSE which indicates that degree 20 leads to the best generalization\n",
        "\n",
        "<font color=”blue”> We need to be careful though, because by choosing the model with the best test performances, we risk to overfit the hyperparameters. Thus, we could observe different performances on new test data. In order to obtain a robust estimation, we would have to split the data in training, validation and test, then select the polynomial degree based on the performances on the validation data, and finally we could correctly estimate the performances of the selected model on the test data, as seen in module 7."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuiosJEX1HOq"
      },
      "source": [
        "### 7.1.3 Nombre de données et capacité des modèles\n",
        "\n",
        "On étudie le comportement des regressions polynomiales et examinons leurs performances quand le nombre de données augmente. En particulier, on étudie le comportement de la regression polynomiale cubique (degré 3) et de la regression polynomiale optimale (celle de degré minimisant l'erreur MSE sur les données de test).\n",
        "\n",
        "**Question**: Exécutez la cellule de code suivante. Est-ce que les courbes obtenues correspondent à ce qui est attendu ? Justifiez.\n",
        "\n",
        "### <font color=”blue”> 7.1.3 Sample size and models' capacity\n",
        "\n",
        "<font color=”blue”> We study the behavior of the polynomial regressors and examine how they perform when the sample size increases. Specifically, we study the behavior of the cubic polynomial regression (degree 3) and the optimal polynomial regression (with the degree that minimizes the MSE on the test set). \n",
        "\n",
        "<font color=”blue”> **Question**: Execute the following code cell. Do the curves behave as expected? Justify."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPur_Obg1HOq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "d9f25093-b83e-4e56-8b60-54c913fff3bf"
      },
      "source": [
        "# Sample size of the training set that we want to study\n",
        "sample_size = [10, 10**2, 10**3, 10**4, 10**5, 10**6]   \n",
        "variance = 20\n",
        "\n",
        "H_train, H_test, optimal_train, optimal_test, optimal_degree \\\n",
        "    = train_poly_and_see(sample_size, scale, period, variance, degree)\n",
        "\n",
        "plot_optimal_curve(optimal_train, optimal_test, H_train, H_test, optimal_degree)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a4e1739b802c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mH_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_degree\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mtrain_poly_and_see\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplot_optimal_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimal_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_degree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_poly_and_see' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QPaLMTlhmca"
      },
      "source": [
        "#### Réponses (essayez d'abord de répondre par vous-même avant de regarder la réponse)\n",
        "\n",
        "#### <font color=”blue”> Answers (try to answer by yourself before looking at the answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K4T_x6Hhqvc"
      },
      "source": [
        "Oui ! Sans avoir à réaliser l'experience, on s'attends à ce que :\n",
        "\n",
        "1. L'erreur MSE sur les données de test diminue lorsque le nombre de données d'entrainement augmente.\n",
        "2. Les courbes d'entrainement et de test coincident quand le nombre de données d'entrainement augmente, parce qu'il est plus difficile d'être en sur-apprentissage lorsque le nombre de données d'entrainement est grand.\n",
        "3. La regression polynomiale cubique a une plus grande erreur MSE (que le modèle optimal) sur les données d'entrainement et de test quand on dispose de suffisament de données d'entrainement (par example $n > 100$).\n",
        "4. Le degré optimal (celui pour lequel la regression polynomiale atteint la plus petite erreur MSE sur les données de test) augmente avec le nombre de données d'entrainement : plus on dispose de données d'entrainement, plus notre modèle devrait avoir une grande capacité.\n",
        "\n",
        "<font color=”blue”> Yes! Without running the experiment, we should expect that:\n",
        "\n",
        "1. The MSE on the test set decreases as the train set gets larger.\n",
        "2. The training and the test curves overlap as the training set gets larger, because it is more difficult to overfit when the training set size is large.\n",
        "3. Cubic polynomial regression has a greater MSE (than the optimal model) on both training and test sets given a large enough training set (say $n > 100$ for example).\n",
        "4. The optimal degree (the one for which polynomial regression achieves the lowest MSE on the test set) increases with the training size : the more training data we have, the more capacity our model should have."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_DEFsrO1HOq"
      },
      "source": [
        "# 7.2 Biais et variance des estimateurs\n",
        "\n",
        "Nous allons maintenant nous intéresser au biais et à la variance de certains estimateurs classiques.\n",
        "\n",
        "# <font color=”blue”> 7.2 Bias and variance of estimators\n",
        "\n",
        "<font color=”blue”> We will now explore some properties of the bias and variance of well-known estimators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jf2nRil1HOq"
      },
      "source": [
        "## Modèles linéaires de régression\n",
        "\n",
        "Le problème d'apprentissage polynomiale vu précédement est un exemple d'une plus grande classe de modèles - les modèles linéaires. Plus précisemment, c'est un problème de régression linéaire, où le but est de prédire la valeur d'une ou plusieurs variables continues étant donné les valeurs de certaines variables d'entrée. Les modèles linéaires partagent la propriété **d'être une fonction linéaire des paramètres appris** (les coefficients du polynome dans le cas des modèles polynomiaux) mais pas nécessairement linéaires en les variables d'entrée.\n",
        "\n",
        "Nous considérons maintenant que notre fonction de perte est l'erreur quadratique (i.e. $\\sum_{i=0}^n (y_i-\\hat{y}_i)^2$ ), et que notre modèle est une fonction linéaire des variables d'entrée.\n",
        "\n",
        "Dans ce contexte, nous pouvons calculer analytiquement les paramètres optimaux de notre modèle pour les données d'entrainement. Ce modèle de regression linéaire entrainé pour minimiser l'erreur quadratique est aussi connu sous le nom de regression ordinary least square (OLS) et a été introduit lors du module 6.\n",
        "\n",
        "Tout d'abord, simulons quelques données.\n",
        "\n",
        "## <font color=”blue”> Linear models for regression\n",
        "\n",
        "<font color=”blue”> The polynomial curve fitting problem encountered previously is an instance of a broader class of models - linear models. More specifically, it is a linear regression task, where the goal is to predict a value of one or more continuous target variables given the values of some input variables. Linear models share the property of **being a linear function of the adjustable parameters** (polynomial coefficients in case of polynomial models) but not necessarily linear in the input variables.\n",
        "\n",
        "<font color=”blue”> We now consider that our loss function is the squared error (i.e. $\\sum_{i=0}^n (y_i-\\hat{y}_i)^2$ ), and that our model is a linear funtion of the input variables. \n",
        "\n",
        "<font color=”blue”> In this setup, we can analytically compute the optimal parameters of the model for the training data. This linear regression model fitted with a squared error is also known as an ordinary least square (OLS) regression and was introduced in module 6.\n",
        "\n",
        "<font color=”blue”> First, let's simulate some data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOMnhITz1HOr"
      },
      "source": [
        "### 7.2.1 Simulation de données\n",
        "\n",
        "Dans cette section, chaque observation $y$ est générée en suivant le modèle suivant:\n",
        "\n",
        "$$ y = \\bf{x}^\\top \\bf{w} + w_0 + \\epsilon$$\n",
        "\n",
        "où $y \\in \\mathbb{R}$ est la variable cible, $\\bf{x}$ est le vecteur de covariables, $\\bf{w_0}$ est un scalaire et $\\epsilon$ est le bruit aléatoire avec $\\epsilon \\sim \\mathcal{N}(0,1)$.\n",
        "\n",
        "### <font color=”blue”> 7.2.1 Data simulation\n",
        "\n",
        "<font color=”blue”> In this section, every observation $y$ is generated according to the following model:\n",
        "\n",
        "<font color=”blue”> $$ y = \\bf{x}^\\top \\bf{w} + w_0 + \\epsilon$$\n",
        "\n",
        "<font color=”blue”> where $y \\in \\mathbb{R}$ is the output, $\\bf{x}$ is the vector of covariates,  $\\bf{w}$ is the vector of the associated weights, $\\bf{w_0}$ is a scalar and $\\epsilon$ is the random noise such as $\\epsilon \\sim \\mathcal{N}(0,1)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpJNhCGd1HOr"
      },
      "source": [
        "def data_simulation(sample_size, w_0, w_1):\n",
        "    \n",
        "    x = np.random.uniform(-1, 10, sample_size)\n",
        "    x.sort()\n",
        "    noise = np.random.normal(0, 1, sample_size)\n",
        "    y = w_0 + w_1 * x + noise\n",
        "    \n",
        "    return x, y"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30c2-LGT1HOs"
      },
      "source": [
        "Étant donné le modèle génératif ci-dessus, nous pouvons maintenant générer les données et les visualiser.\n",
        "\n",
        "<font color=”blue”> Given the above generative model, we can now sample the data and visualize it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAMZInXe1HOs"
      },
      "source": [
        "w_0, w_1 = 2, 3   # Fix w values\n",
        "sample_size = 500   # Fix the sample size - train\n",
        "\n",
        "X, y = data_simulation(sample_size, w_0, w_1)\n",
        "X = [np.ones(len(y)), X] # We fill the first column of X with ones, as explained below\n",
        "X = np.asarray(X ).T\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "scatter_plot(X_train[:, 1], X_test [:, 1], y_train, y_test) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqCu-irb1HOs"
      },
      "source": [
        "### 7.2.2 Calcul des estimateurs OLS\n",
        "\n",
        "Durant le cours, nous avons vu que l'estimateur OLS est obtenu par l'expression suivante :\n",
        "\n",
        "$$ \\hat{\\bf{w}}^{\\text{OLS}} := (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\bf{y}$$\n",
        "\n",
        "où $\\bf{X}$ est la [matrice de conception](https://en.wikipedia.org/wiki/Design_matrix#:~:text=In%20statistics%2C%20a%20design%20matrix,specific%20values%20for%20that%20object.) et $\\bf{y}$ est le vecteur de variables cibles. Notez qu'afin de calculer le terme $\\bf{w_0}$, nous avons rempli la première colonne de $\\bf{X}$ avec des uns (dans la cellule de code précédente), comme explicitement suggéré [ici](https://en.wikipedia.org/wiki/Design_matrix#:~:text=In%20statistics%2C%20a%20design%20matrix,specific%20values%20for%20that%20object.).\n",
        "\n",
        "**Question**:\n",
        "\n",
        "Remplissez la cellule de code suivante pour calculer l'estimateur OLS basé sur l'expression ci-dessus.\n",
        "\n",
        "### <font color=”blue”> 7.2.2 Computation of the OLS estimators\n",
        "\n",
        "<font color=”blue”> During the course, we have seen that the OLS estimator is given by the following expression :\n",
        "\n",
        "<font color=”blue”> $$ \\hat{\\bf{w}}^{\\text{OLS}} := (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\bf{y}$$\n",
        "\n",
        "<font color=”blue”> where $\\bf{X}$ is the [design matrix](https://en.wikipedia.org/wiki/Design_matrix#:~:text=In%20statistics%2C%20a%20design%20matrix,specific%20values%20for%20that%20object.) and $\\bf{y}$ is the output vector. Note that in order to compute the term $\\bf{w_0}$, we have filled the first column of $\\bf{X}$ with ones (in the previous code cell), as explicitely suggested [here](https://en.wikipedia.org/wiki/Design_matrix#:~:text=In%20statistics%2C%20a%20design%20matrix,specific%20values%20for%20that%20object.).\n",
        "\n",
        "<font color=”blue”> **Question**: \n",
        "\n",
        "<font color=”blue”> Fill the following code cell to compute the OLS estimator based on the expression above.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93RHjR0f1HOs"
      },
      "source": [
        "def OLS(X, y):  \n",
        "    \"\"\"\n",
        "    X: design matrix\n",
        "    y: output vector\n",
        "    \n",
        "    return:  array of weights\n",
        "    \"\"\"\n",
        "        \n",
        "    return (...) # this should return the vector w_OLS\n",
        "\n",
        "w_ols = OLS(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u54_90R_u8dZ"
      },
      "source": [
        "#### Réponses (essayez d'abord de répondre par vous-même avant de regarder la réponse)\n",
        "\n",
        "#### <font color=”blue”> Answers (try to answer by yourself before looking at the answers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miwYKfk4vul_"
      },
      "source": [
        "def OLS(X, y):\n",
        "    \n",
        "    A = np.linalg.inv(np.dot(X.T, X))\n",
        "    B = np.dot(X.T, y)\n",
        "    \n",
        "    return np.dot(A, B)\n",
        "\n",
        "w_ols = OLS(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfuzJtCN1HOt"
      },
      "source": [
        "#### **Question**: \n",
        "\n",
        "Remplissez la cellule de code suivante pour effectuer les prédictions sur les données d'entrainement et de test et calculer l'erreur MSE associée.\n",
        "\n",
        "#### <font color=”blue”> **Question**: \n",
        "\n",
        "<font color=”blue”> Fill the following code cell to make predictions on training and test sets and compute the associated MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDNlgE5v1HOt"
      },
      "source": [
        "# Train set\n",
        "y_hat_train = (...) # Compute here the predictions on training data\n",
        "\n",
        "# Test set\n",
        "y_hat_test = (...) # Compute here the predictions on test data\n",
        "\n",
        "print('MSE of the train: ', MSE(y_hat_train, y_train))\n",
        "print('MSE of the test:  ', MSE(y_hat_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqva_EEcwhSd"
      },
      "source": [
        "#### Réponses (essayez d'abord de répondre par vous-même avant de regarder la réponse)\n",
        "\n",
        "#### <font color=”blue”> Answers (try to answer by yourself before looking at the answers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aI9-ggEwkap"
      },
      "source": [
        "# Train set\n",
        "y_hat_train = np.dot(X_train, w_ols)\n",
        "\n",
        "# Test set\n",
        "y_hat_test = np.dot(X_test, w_ols)\n",
        "\n",
        "print('MSE of the train: ', MSE(y_hat_train, y_train))\n",
        "print('MSE of the test:  ', MSE(y_hat_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eSdPNff1HOv"
      },
      "source": [
        "### 7.2.3 Biais et variance des estimateurs OLS\n",
        "\n",
        "L'estimateur OLS calculé dans la section précédente dépends des données générées et peut donc être vu comme une variable aléatoire. Par conséquent cet estimateur a une moyenne et une variance. Le biais de cet estimateur est simplement la différence entre sa moyenne et le $\\bf{w}$ du modèle générant les données.\n",
        "\n",
        "Il y a deux approches pour estimer le biais et la variance de l'estimateur OLS. Premièrement, il est possible de le calculer analytiquement (voir section 7.4 - Pour aller plus loin). Une autre méthode est de les estimer statistiquement : on génère plusieurs sets de variables de sortie $y$ et calculons pour chacun l'estimateur OLS correspondant. On calcule ensuite la moyenne des estimateurs obtenus (et en déduisons le biais) et leur variance. Plus on calcule un grand nombre d'estimateurs, plus ces estimations seront fiables. Note : le biais et la variance de l'estimateur OLS dépendent de la matrice de conception $\\bf{X}$, qui est fixée dans notre cas (seul les $y$ peuvent changer lorsque nous générons les données).\n",
        "\n",
        "**Question**: Remplissez la cellule de code ci-dessous en suivant les indications pour estimer le biais et la variance des estimateurs OLS. Que pouvez vous dire des résultats ?\n",
        "\n",
        "### <font color=”blue”> 7.2.3 Bias and variance of the OLS estimators\n",
        "\n",
        "<font color=”blue”> The OLS estimator calculated in the last section depends of the generated data and thus can be seen as a random variable. It follows that this estimator has a mean and variance. The bias of this estimator is simply the difference between its mean and the $\\bf{w}$ of the data-generating model.\n",
        "\n",
        "<font color=”blue”> There are two approaches to estimate the bias and variance of the OLS estimator. First, it is possible to compute it analytically in closed form (see section 7.4 - To go further). Another method is to estimate them statistically : we generate different sets of output data $y$ and compute each corresponding OLS estimator. We can then compute the mean of these estimators (and deduce the bias from it) and their variance. The more estimators we compute, the more reliable these estimations will be. Note : the bias and variance of the OLS estimator depend of the design matrix $\\bf{X}$, which is fixed in our case (only the $y$ may change when we sample data).\n",
        "\n",
        "<font color=”blue”> **Question**: Fill the following code cell following the indications to estimate the bias and variance of the OLS estimators. What can you tell about the results ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhGvLWSz1HOv"
      },
      "source": [
        "mc_estimates = 1000 # Number of estimators that you want to compute\n",
        "M = np.zeros((mc_estimates, 2)) # Matrix to store the estimators\n",
        "\n",
        "# Step 1: Create a loop \n",
        "for k in np.arange(mc_estimates):\n",
        "    \n",
        "    # Step 2: Data simulation for k-th estimator\n",
        "    x, y = data_simulation( int(.8 * sample_size), w_0, w_1)\n",
        "    \n",
        "    X = [np.ones(len(y)), x]   \n",
        "    X = np.asarray(X).T\n",
        "    \n",
        "    # Step 3: Compute the k-th estimator\n",
        "    w_ols = (...) # Compute the OLS estimator here, using a function you filled previously\n",
        "    M[k, :] = w_ols # Store the estimator\n",
        "     \n",
        "# Step 4: Compute the bias and variance of w_0 and w_1 (hint: np.var)\n",
        "mean = (...) # Compute here the means of w_0 and w_1\n",
        "bias = (...) # Compute here the biases of w_0 and w_1\n",
        "var = (...) # Compute here the variances of w_0 and w_1\n",
        "\n",
        "print(\"MC estimate of the bias of the w_0 estimate: \", (...))   # Print here the bias of w_0\n",
        "print(\"MC estimate of the bias of the w_1 estimate: \", (...))   # Print here the bias of w_1\n",
        "print(\"MC estimate of the variance of the w_0 estimate: \", (...))   # Print here the variance of w_0\n",
        "print(\"MC estimate of the variance of the w_1 estimate: \", (...))   # Print here the variance of w_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9E9i9bff3f18"
      },
      "source": [
        "#### Réponses (essayez d'abord de répondre par vous-même avant de regarder la réponse)\n",
        "\n",
        "#### <font color=”blue”> Answers (try to answer by yourself before looking at the answers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Iokk6i23hK8"
      },
      "source": [
        "mc_estimates = 1000 # Number of estimators that you want to compute\n",
        "M = np.zeros((mc_estimates, 2)) # Matrix to store the estimators\n",
        "\n",
        "# Step 1: Create a loop \n",
        "for k in np.arange(mc_estimates):\n",
        "    \n",
        "    # Step 2: Data simulation for k-th estimator\n",
        "    x, y = data_simulation( int(.8 * sample_size), w_0, w_1)\n",
        "    \n",
        "    X = [np.ones(len(y)), x]   \n",
        "    X = np.asarray(X).T\n",
        "    \n",
        "    # Step 3: Compute the k-th estimator\n",
        "    w_ols = OLS(X, y) # Compute the OLS estimator here, using a function you filled previously\n",
        "    M[k, :] = w_ols # Store the estimator\n",
        "    \n",
        "# Step 4: Compute the bias and variance of w_0 and w_1 (hint: np.var)\n",
        "mean_est = np.mean(M, axis=0) # Compute here the means of w_0 and w_1\n",
        "bias = mean_est - [w_0, w_1] # Compute here the biases of w_0 and w_1\n",
        "var = np.var(M, axis=0) # Compute here the variances of w_0 and w_1\n",
        "\n",
        "print(\"MC estimate of the bias of the w_0 estimate: \", bias[0])   # Print here the bias of w_0\n",
        "print(\"MC estimate of the bias of the w_1 estimate: \", bias[1])   # Print here the bias of w_1\n",
        "print(\"MC estimate of the variance of the w_0 estimate: \", var[0])   # Print here the variance of w_0\n",
        "print(\"MC estimate of the variance of the w_1 estimate: \", var[1])   # Print here the variance of w_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVihRu_19JEQ"
      },
      "source": [
        "Normalement, les variances et biais calculés devraient être très faibles, indiquant que l'estimateur OLS est très adapté à ce problème de regression. En fait, il est possible de calculer analytiquement le biais et la variance pour confirmer ces résultats. C'est ce que nous faisons dans la section suivante.\n",
        "\n",
        "<font color=”blue”> The computed variances and biases should be very low, indicating that the OLS estimator is well suited for this regression problem. In fact, it is possible to analytically compute the bias and variance to confirm these results. This is what we will do in the following section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlmzAGl5-KGm"
      },
      "source": [
        "## 7.4 Pour aller plus loin\n",
        "\n",
        "On s'intéresse maintenant aux valeurs théoriques du biais et de la variance qui peuvent être calculées dans le cas de l'estimateur OLS.\n",
        "\n",
        "## <font color=”blue”> 7.4 To go further\n",
        "\n",
        "<font color=”blue”> We now focus on the theoretical values of the bias and variance that can be computed for the OLS estimator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL1F8xLQ-fp8"
      },
      "source": [
        "### 7.4.1 Calcul analytique du biais\n",
        "\n",
        "Le calcul suivant permets d'obtenir la moyenne de l'estimateur OLS, et on observe immédiatement qu'il est non-biaisé ! En effet, $\\mathbb{E}[\\hat{\\bf{w}}^{\\text{OLS}}-\\bf{w}] = 0$\n",
        "\n",
        "Cela justifie le faible biais estimé statistiquement dans la section précédente. \n",
        "\n",
        "### <font color=”blue”> 7.4.1 Analytical computation of the bias\n",
        "\n",
        "<font color=”blue”> The following computation gives us the mean of the OLS estimator, and we can immediately see that it is unbiased ! Indeed, $\\mathbb{E}[\\hat{\\bf{w}}^{\\text{OLS}}-\\bf{w}] = 0$\n",
        "\n",
        "<font color=”blue”> This justifies the low bias statistically estimated in the previous section.\n",
        "\n",
        "\\begin{align}\n",
        "    \\mathbb{E}[\\hat{\\bf{w}}^{\\text{OLS}}] \n",
        "    &= \\mathbb{E}[(\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\bf{y}] \\\\\n",
        "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\mathbb{E}[\\bf{y}] \\\\\n",
        "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\mathbb{E}[\\bf{X} \\bf{w} + \\bf{\\epsilon}] \\\\\n",
        "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\{ \\mathbb{E}[\\bf{X} \\bf{w}] + \\mathbb{E}[\\bf{\\epsilon}] \\} \\\\\n",
        "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\{ \\bf{X} \\bf{w} + \\bf{O} \\} \\\\\n",
        "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\bf{X} \\bf{w}  \\\\\n",
        "    &= \\bf{w}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiYvCwJP1HOv"
      },
      "source": [
        "### 7.4.2 Calcul analytique de la variance\n",
        "\n",
        "Le calcul suivant nous permets d'obtenir la variance de l'estimateur OLS en fonction de la matrice de conception $\\bf{X}$.\n",
        "\n",
        "**Question**: remplissez la cellule de code suivante pour calculer la variance théorique de l'estimateur OLS. Comparez la à la variance estimée dans la section 7.3.\n",
        "\n",
        "### <font color=”blue”> 7.4.2 Analytical computation of the variance\n",
        "\n",
        "<font color=”blue”> The following computation gives us the variance of the OLS estimator, as a function of the design matrix $\\bf{X}$.\n",
        "\n",
        "<font color=”blue”> **Question**: fill the following code cell to compute the theoretical variance of the OLS estimator. Compare it to the variance estimated in section 7.3.\n",
        "\n",
        "\\begin{align}\n",
        "    \\text{Var}(\\hat{\\bf{w}}^{\\text{OLS}}) \n",
        "    &= \\text{Var}\\left((\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\bf{y}\\right) && \\text{Definition of the OLS estimators}  \\\\\n",
        "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\text{Var} (\\bf{y})\\left((\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\right)^\\top && \\text{Property of the variance on matrices} \\\\\n",
        "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\text{Var} (\\bf{y}) \\bf{X}\\left((\\bf{X}^\\top \\bf{X})^{-1} \\right)^\\top && \\text{Property of the transpose}  \\\\\n",
        "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\text{Var}(\\bf{X} \\bf{w} + \\bf{\\epsilon}) \\ \\bf{X}\\left((\\bf{X}^\\top \\bf{X})^{-1} \\right)^\\top && \\text{Modelization of the data}\\\\\n",
        "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\text{Var}(\\bf{\\epsilon}) \\ \\bf{X}\\left((\\bf{X}^\\top \\bf{X})^{-1} \\right)^\\top && \\text{Property of the variance (only $\\epsilon$ is a random variable)} \\\\\n",
        "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top \\bf{I} \\ \\bf{X}\\left((\\bf{X}^\\top \\bf{X})^{-1} \\right)^\\top && \\text{Since $\\epsilon$ is iid}\\\\\n",
        "    &= (\\bf{X}^\\top \\bf{X})^{-1} \\bf{X}^\\top  \\bf{X}\\left((\\bf{X}^\\top \\bf{X})^{-1} \\right)^\\top \\\\\n",
        "    &= \\left((\\bf{X}^\\top \\bf{X})^{-1} \\right)^\\top \\\\\n",
        "    &= (\\bf{X}^\\top \\bf{X})^{-1} && \\text{Since $\\bf{X}^\\top \\bf{X}$ is symetric and by property of the inverse of a matrix}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMXkReTv1HOw"
      },
      "source": [
        "# Computation\n",
        "var = (...) # Use the expression above on X_train\n",
        "\n",
        "# Interest\n",
        "print(\"Analytical variance of the w_0 estimate: \", (...))   \n",
        "print(\"Analytical variance of the w_1 estimate: \", (...))   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvRdl7_jBare"
      },
      "source": [
        "#### Réponses (essayez d'abord de répondre par vous-même avant de regarder la réponse)\n",
        "\n",
        "#### <font color=”blue”> Answers (try to answer by yourself before looking at the answers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdsqt4JwBddg"
      },
      "source": [
        "# Computation\n",
        "var = np.linalg.inv(np.dot(X_train.T, X_train)) # Use the expression above on X_train\n",
        "\n",
        "# Interest\n",
        "print(\"Analytical variance of the w_0 estimate: \", var[0, 0])   \n",
        "print(\"Analytical variance of the w_1 estimate: \",var[1, 1])   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uTwhuc0BdDM"
      },
      "source": [
        "On peut voir comme attendu que la valeur théorique de la variance est similaire à celle obtenue section 7.3.\n",
        "\n",
        "<font color=”blue”> We can see that the theoretical value of variance is, as expected, similar to the one obtained in section 7.3."
      ]
    }
  ]
}